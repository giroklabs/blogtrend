from flask import Flask, render_template, jsonify, request
import requests
import json
import os
from datetime import datetime, timedelta
from pytrends.request import TrendReq
import pandas as pd
from dotenv import load_dotenv
import time

load_dotenv()

app = Flask(__name__)

# GitHub API ì„¤ì •
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')
GITHUB_API_BASE = 'https://api.github.com'

# Google Trends ì„¤ì •
pytrends = TrendReq(hl='ko-KR', tz=540)  # í•œêµ­ ì‹œê°„ëŒ€

class BlogAnalyzer:
    def __init__(self):
        self.github_headers = {
            'Authorization': f'token {GITHUB_TOKEN}' if GITHUB_TOKEN else {},
            'Accept': 'application/vnd.github.v3+json'
        }
    
    def get_github_repos(self, username):
        """GitHub ì‚¬ìš©ìì˜ ë¸”ë¡œê·¸ ê´€ë ¨ ì €ì¥ì†Œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            url = f"{GITHUB_API_BASE}/users/{username}/repos"
            response = requests.get(url, headers=self.github_headers)
            response.raise_for_status()
            
            repos = response.json()
            blog_repos = []
            
            for repo in repos:
                # ë¸”ë¡œê·¸ ê´€ë ¨ ì €ì¥ì†Œ í•„í„°ë§
                if any(keyword in repo['name'].lower() for keyword in ['blog', 'blogger', 'website', 'site']):
                    blog_repos.append({
                        'name': repo['name'],
                        'description': repo['description'],
                        'stars': repo['stargazers_count'],
                        'forks': repo['forks_count'],
                        'url': repo['html_url'],
                        'created_at': repo['created_at'],
                        'updated_at': repo['updated_at']
                    })
            
            return blog_repos
        except Exception as e:
            print(f"GitHub API ì˜¤ë¥˜: {e}")
            return []
    
    def get_trending_topics(self, keywords, timeframe='today 12-m'):
        """Google Trendsì—ì„œ í‚¤ì›Œë“œë“¤ì˜ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # í‚¤ì›Œë“œë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(keywords, str):
                keywords = [keywords]
            
            # Google Trends ìš”ì²­
            pytrends.build_payload(keywords, cat=0, timeframe=timeframe, geo='KR')
            
            # ê´€ì‹¬ë„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            interest_over_time = pytrends.interest_over_time()
            
            # ê´€ë ¨ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸°
            related_queries = pytrends.related_queries()
            
            # DataFrameì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_related_queries = {}
            for keyword, queries in related_queries.items():
                serializable_related_queries[keyword] = {}
                if queries.get('top') is not None:
                    serializable_related_queries[keyword]['top'] = queries['top'].to_dict('records') if not queries['top'].empty else []
                else:
                    serializable_related_queries[keyword]['top'] = []
                
                if queries.get('rising') is not None:
                    serializable_related_queries[keyword]['rising'] = queries['rising'].to_dict('records') if not queries['rising'].empty else []
                else:
                    serializable_related_queries[keyword]['rising'] = []
            
            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            interest_data = interest_over_time.to_dict('records') if not interest_over_time.empty else []
            
            # ê° ë°ì´í„° í¬ì¸íŠ¸ì— ë‚ ì§œ ì •ë³´ ì¶”ê°€
            if interest_data:
                # Google TrendsëŠ” ë³´í†µ ìµœê·¼ 12ê°œì›” ë°ì´í„°ë¥¼ ì œê³µ
                from datetime import datetime, timedelta
                end_date = datetime.now()
                start_date = end_date - timedelta(days=365)
                
                # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì— ë”°ë¼ ë‚ ì§œ ë¶„ë°°
                for i, data_point in enumerate(interest_data):
                    if len(interest_data) > 1:
                        days_back = (len(interest_data) - 1 - i) * (365 // (len(interest_data) - 1))
                    else:
                        days_back = 0
                    
                    point_date = end_date - timedelta(days=days_back)
                    data_point['date'] = point_date.strftime('%Y-%m-%d')
            
            return {
                'interest_over_time': interest_data,
                'related_queries': serializable_related_queries
            }
        except Exception as e:
            print(f"Google Trends API ì˜¤ë¥˜: {e}")
            return {'interest_over_time': [], 'related_queries': {}}
    
    def get_trending_searches(self, timeframe='today 1-d'):
        """Google Trendsì—ì„œ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # ì¸ê¸° ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° (í•œêµ­)
            trending_searches = pytrends.trending_searches(pn='south_korea')
            
            return {
                'trending_searches': trending_searches.tolist() if not trending_searches.empty else []
            }
        except Exception as e:
            print(f"Google Trends ì¸ê¸° ê²€ìƒ‰ì–´ API ì˜¤ë¥˜: {e}")
            # ê¸°ê°„ë³„ ê°€ìƒ ë°ì´í„° ë°˜í™˜
            if '1-d' in timeframe:
                return {
                    'trending_searches': [
                        'íŒŒì´ì¬', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'ë¦¬ì•¡íŠ¸', 'ì›¹ê°œë°œ', 'ì½”ë”©',
                        'ê°œë°œì', 'í”„ë¡œê·¸ë˜ë°', 'ê¹ƒí—ˆë¸Œ', 'í‹°ìŠ¤í† ë¦¬', 'ë„¤ì´ë²„ë¸”ë¡œê·¸',
                        'ë²¨ë¡œê·¸', 'ë¯¸ë””ì—„', 'ê°œë°œë„êµ¬', 'IDE', 'VS Code',
                        'ë°ì´í„°ë¶„ì„', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'í´ë¼ìš°ë“œ', 'DevOps'
                    ]
                }
            elif '7-d' in timeframe:
                return {
                    'trending_searches': [
                        'ë¸”ë¡œê·¸ í”Œë«í¼', 'ê°œë°œì ì»¤ë®¤ë‹ˆí‹°', 'ê¸°ìˆ  ë¸”ë¡œê·¸', 'í”„ë¡œê·¸ë˜ë° ì–¸ì–´', 'ì›¹ ê°œë°œ',
                        'ëª¨ë°”ì¼ ì•± ê°œë°œ', 'ë°ì´í„° ë¶„ì„', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ ì»´í“¨íŒ…', 'ë³´ì•ˆ',
                        'DevOps', 'ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤', 'ë„ì»¤', 'ì¿ ë²„ë„¤í‹°ìŠ¤', 'CI/CD',
                        'í…ŒìŠ¤íŠ¸ ìë™í™”', 'ì½”ë“œ ë¦¬ë·°', 'í˜ì–´ í”„ë¡œê·¸ë˜ë°', 'ì• ìì¼', 'ìŠ¤í¬ëŸ¼'
                    ]
                }
            elif '1-m' in timeframe:
                return {
                    'trending_searches': [
                        'ìŠ¤í”„ë§ë¶€íŠ¸', 'ì¥ê³ ', 'í”Œë¼ìŠ¤í¬', 'ë…¸ë“œjs', 'ìµìŠ¤í”„ë ˆìŠ¤',
                        'ë·°js', 'ì•µê·¤ëŸ¬', 'íƒ€ì…ìŠ¤í¬ë¦½íŠ¸', 'ê³ ë­', 'ëŸ¬ìŠ¤íŠ¸',
                        'ì½”í‹€ë¦°', 'ìŠ¤ìœ„í”„íŠ¸', 'í”ŒëŸ¬í„°', 'ë¦¬ì•¡íŠ¸ë„¤ì´í‹°ë¸Œ', 'ìœ ë‹ˆí‹°',
                        'ë¸”ë Œë”', 'í”¼ê·¸ë§ˆ', 'ì œí”Œë¦°', 'ë…¸ì…˜', 'ìŠ¬ë™'
                    ]
                }
            else:  # 12-m (ì—°ê°„)
                return {
                    'trending_searches': [
                        'ë©”íƒ€ë²„ìŠ¤', 'NFT', 'ë¸”ë¡ì²´ì¸', 'Web3', 'DeFi',
                        'í¬ë¦½í† ', 'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ì†”ë¼ë‚˜', 'í´ë¦¬ê³¤',
                        'ë””ì•±', 'ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸', 'DAO', 'DeFi í”„ë¡œí† ì½œ', 'NFT ë§ˆì¼“í”Œë ˆì´ìŠ¤',
                        'ê²Œì„íŒŒì´', 'í”Œë ˆì´íˆ¬ì–¸', 'ìŠ¤í…Œì´í‚¹', 'ë¦¬í€´ë””í‹°', 'ê°€ìŠ¤ë¹„'
                    ]
                }
    
    def get_top_charts(self, timeframe='today 1-d'):
        """Google Trendsì—ì„œ ìƒìœ„ ì°¨íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # ìƒìœ„ ì°¨íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì—°ë„ í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •)
            current_year = datetime.now().year
            top_charts = pytrends.top_charts(date=str(current_year), hl='ko-KR', tz=540, geo='KR')
            
            return {
                'top_charts': top_charts.to_dict('records') if not top_charts.empty else []
            }
        except Exception as e:
            print(f"Google Trends ìƒìœ„ ì°¨íŠ¸ API ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê°€ìƒ ë°ì´í„° ë°˜í™˜
            return {
                'top_charts': [
                    {'title': 'ë¸”ë¡œê·¸ í”Œë«í¼', 'traffic': 'ë†’ìŒ'},
                    {'title': 'ê°œë°œì ì»¤ë®¤ë‹ˆí‹°', 'traffic': 'ë†’ìŒ'},
                    {'title': 'í”„ë¡œê·¸ë˜ë° ì–¸ì–´', 'traffic': 'ì¤‘ê°„'},
                    {'title': 'ì›¹ ê°œë°œ', 'traffic': 'ë†’ìŒ'},
                    {'title': 'ëª¨ë°”ì¼ ì•± ê°œë°œ', 'traffic': 'ì¤‘ê°„'},
                    {'title': 'ë°ì´í„° ë¶„ì„', 'traffic': 'ë†’ìŒ'},
                    {'title': 'ì¸ê³µì§€ëŠ¥', 'traffic': 'ë†’ìŒ'},
                    {'title': 'í´ë¼ìš°ë“œ ì»´í“¨íŒ…', 'traffic': 'ì¤‘ê°„'},
                    {'title': 'ë³´ì•ˆ', 'traffic': 'ì¤‘ê°„'},
                    {'title': 'DevOps', 'traffic': 'ì¤‘ê°„'}
                ]
            }
    
    def get_daily_trends(self):
        """ì¼ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        return self.get_trending_searches('today 1-d')
    
    def get_weekly_trends(self):
        """ì£¼ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        return self.get_trending_searches('today 7-d')
    
    def get_monthly_trends(self):
        """ì›”ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        return self.get_trending_searches('today 1-m')
    
    def get_yearly_trends(self):
        """ì—°ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        return self.get_trending_searches('today 12-m')
    
    def analyze_blog_traffic(self, blog_urls):
        """ë¸”ë¡œê·¸ URLë“¤ì˜ íŠ¸ë˜í”½ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print(f"analyze_blog_traffic í˜¸ì¶œë¨. ì…ë ¥: {blog_urls}")
        blog_keywords = []
        
        for url in blog_urls:
            # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
            domain = url.replace('https://', '').replace('http://', '').split('/')[0]
            # www. ì œê±°
            domain = domain.replace('www.', '')
            blog_keywords.append(domain)
            print(f"ì²˜ë¦¬ëœ í‚¤ì›Œë“œ: {domain}")
        
        # í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not blog_keywords:
            blog_keywords = ['example']
            print("í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆì–´ì„œ ê¸°ë³¸ê°’ ì„¤ì •")
        
        print(f"ìµœì¢… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {blog_keywords}")
        
        try:
            # íŠ¸ë Œë“œ ë¶„ì„
            print("get_trending_topics í˜¸ì¶œ...")
            trends_data = self.get_trending_topics(blog_keywords)
            print(f"íŠ¸ë Œë“œ ë°ì´í„°: {trends_data}")
            
            # ë¶„ì„ ê²°ê³¼ì— ë„ë©”ì¸ ì •ë³´ ì¶”ê°€
            trends_data['analyzed_domains'] = blog_keywords
            
            return trends_data
        except Exception as e:
            print(f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            
            # ì˜¤ë¥˜ ì‹œ ê°€ìƒ ë°ì´í„° ë°˜í™˜ (ì‹¤ì œ í‚¤ì›Œë“œ ì‚¬ìš©)
            print("ê°€ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
            interest_data = []
            
            # ìµœê·¼ 3ê°œì›” ë°ì´í„° ìƒì„±
            from datetime import datetime, timedelta
            end_date = datetime.now()
            
            for i in range(3):  # 3ê°œì›” ë°ì´í„°
                point_date = end_date - timedelta(days=(2-i)*30)  # ìµœê·¼ 3ê°œì›”
                data_point = {'date': point_date.strftime('%Y-%m-%d')}
                for j, keyword in enumerate(blog_keywords[:3]):  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                    data_point[f'keyword{j+1}'] = 50 + (i * 10) + (j * 5)
                interest_data.append(data_point)
            
            related_queries = {}
            for keyword in blog_keywords[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                related_queries[keyword] = {
                    'top': [
                        {'query': f'{keyword} ê´€ë ¨ ê²€ìƒ‰ì–´ 1', 'value': 100},
                        {'query': f'{keyword} ê´€ë ¨ ê²€ìƒ‰ì–´ 2', 'value': 80},
                        {'query': f'{keyword} ê´€ë ¨ ê²€ìƒ‰ì–´ 3', 'value': 60}
                    ]
                }
            
            result = {
                'interest_over_time': interest_data,
                'related_queries': related_queries,
                'analyzed_domains': blog_keywords
            }
            print(f"ê°€ìƒ ë°ì´í„° ê²°ê³¼: {result}")
            return result
    
    def get_popular_blogs(self):
        """ì¸ê¸° ë¸”ë¡œê·¸ í”Œë«í¼ë“¤ì˜ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        popular_blogs = [
            'tistory.com',
            'blog.naver.com', 
            'blog.daum.net',
            'medium.com',
            'velog.io',
            'github.io'
        ]
        
        return self.get_trending_topics(popular_blogs)

# ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
analyzer = BlogAnalyzer()

# AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ë¡œë”©)
ai_generator = None

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/robots.txt')
def robots():
    return app.send_static_file('robots.txt')

@app.route('/sitemap.xml')
def sitemap():
    return app.send_static_file('sitemap.xml')

@app.route('/verification.html')
def verification():
    return app.send_static_file('verification.html')



@app.route('/api/trends')
def trends_analysis():
    """ì¸ê¸° ë¸”ë¡œê·¸ í”Œë«í¼ íŠ¸ë Œë“œ ë¶„ì„"""
    try:
        trends = analyzer.get_popular_blogs()
        return jsonify({
            'success': True,
            'trends': trends
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trends/daily')
def daily_trends():
    """ì¼ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œ"""
    try:
        trends = analyzer.get_daily_trends()
        return jsonify({
            'success': True,
            'trends': trends,
            'period': 'daily'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trends/week')
def weekly_trends():
    """ì£¼ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œ"""
    try:
        trends = analyzer.get_weekly_trends()
        return jsonify({
            'success': True,
            'trends': trends,
            'period': 'weekly'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trends/month')
def monthly_trends():
    """ì›”ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œ"""
    try:
        trends = analyzer.get_monthly_trends()
        return jsonify({
            'success': True,
            'trends': trends,
            'period': 'monthly'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trends/year')
def yearly_trends():
    """ì—°ê°„ ë² ìŠ¤íŠ¸ íŠ¸ë Œë“œ"""
    try:
        trends = analyzer.get_yearly_trends()
        return jsonify({
            'success': True,
            'trends': trends,
            'period': 'yearly'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500



@app.route('/api/analyze', methods=['POST'])
def analyze_custom_blogs():
    """ì‚¬ìš©ì ì •ì˜ ë¸”ë¡œê·¸ URL ë¶„ì„"""
    try:
        print("=== API í˜¸ì¶œ ì‹œì‘ ===")
        data = request.get_json()
        print(f"ë°›ì€ ë°ì´í„°: {data}")
        
        blog_urls = data.get('urls', [])
        print(f"ì¶”ì¶œëœ URLë“¤: {blog_urls}")
        
        if not blog_urls:
            print("URLì´ ë¹„ì–´ìˆìŒ")
            return jsonify({
                'success': False,
                'error': 'ë¸”ë¡œê·¸ URLì´ í•„ìš”í•©ë‹ˆë‹¤.'
            }), 400
        
        print("ë¶„ì„ ì‹œì‘...")
        analysis = analyzer.analyze_blog_traffic(blog_urls)
        print(f"ë¶„ì„ ê²°ê³¼: {analysis}")
        
        return jsonify({
            'success': True,
            'analysis': analysis
        })
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/generate-blog', methods=['POST'])
def generate_blog_content():
    """ì›¹ í¬ë¡¤ë§ ê¸°ë°˜ ë¸”ë¡œê·¸ ì •ë³´ ìˆ˜ì§‘ê¸°"""
    try:
        data = request.get_json()
        keyword = data.get('keyword', '')
        
        if not keyword:
            return jsonify({
                'success': False,
                'error': 'í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
            }), 400
        
        # ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ì •ë³´ ìˆ˜ì§‘
        try:
            import requests
            from bs4 import BeautifulSoup
            import re
            
            # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
            search_results = []
            
            # Google ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ (ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ API ì‚¬ìš© ê¶Œì¥)
            search_url = f"https://www.google.com/search?q={keyword}+ë¸”ë¡œê·¸+ê°€ì´ë“œ"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            try:
                response = requests.get(search_url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
                search_divs = soup.find_all('div', class_='g')
                for div in search_divs[:5]:  # ìƒìœ„ 5ê°œ ê²°ê³¼
                    title_elem = div.find('h3')
                    snippet_elem = div.find('div', class_='VwiC3b')
                    
                    if title_elem and snippet_elem:
                        search_results.append({
                            'title': title_elem.get_text(),
                            'snippet': snippet_elem.get_text()[:200] + '...'
                        })
            except Exception as e:
                print(f"ê²€ìƒ‰ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
            # í‚¤ì›Œë“œë³„ ê¸°ë³¸ ì •ë³´ ìƒì„±
            keyword_info = {
                'íŒŒì´ì¬': {
                    'description': 'Pythonì€ ê°„ë‹¨í•˜ê³  ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, AI, ìë™í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.',
                    'features': ['ê°„ë‹¨í•œ ë¬¸ë²•', 'í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬', 'í¬ë¡œìŠ¤ í”Œë«í¼', 'ì˜¤í”ˆì†ŒìŠ¤'],
                    'learning_path': ['ê¸°ë³¸ ë¬¸ë²•', 'í•¨ìˆ˜ì™€ í´ë˜ìŠ¤', 'íŒŒì¼ ì²˜ë¦¬', 'ì›¹ í”„ë ˆì„ì›Œí¬', 'ë°ì´í„° ë¶„ì„']
                },
                'ë¦¬ì•¡íŠ¸': {
                    'description': 'ReactëŠ” Facebookì—ì„œ ê°œë°œí•œ JavaScript ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì„ ì–¸ì ì´ê³  íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.',
                    'features': ['ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜', 'ê°€ìƒ DOM', 'ë‹¨ë°©í–¥ ë°ì´í„° íë¦„', 'JSX'],
                    'learning_path': ['JavaScript ê¸°ì´ˆ', 'JSX ë¬¸ë²•', 'ì»´í¬ë„ŒíŠ¸', 'State ê´€ë¦¬', 'Hooks']
                },
                'ìë°”ìŠ¤í¬ë¦½íŠ¸': {
                    'description': 'JavaScriptëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë™ì ì¸ ì›¹ í˜ì´ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.',
                    'features': ['í”„ë¡œí† íƒ€ì… ê¸°ë°˜', 'ë™ì  íƒ€ì…', 'ì´ë²¤íŠ¸ ê¸°ë°˜', 'ë¹„ë™ê¸° ì²˜ë¦¬'],
                    'learning_path': ['ê¸°ë³¸ ë¬¸ë²•', 'DOM ì¡°ì‘', 'ì´ë²¤íŠ¸ ì²˜ë¦¬', 'AJAX', 'ES6+']
                },
                'ì›¹ê°œë°œ': {
                    'description': 'ì›¹ ê°œë°œì€ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ ê°œë°œì„ í¬í•¨í•©ë‹ˆë‹¤.',
                    'features': ['HTML/CSS/JavaScript', 'ë°˜ì‘í˜• ë””ìì¸', 'ì›¹ í‘œì¤€', 'ì„±ëŠ¥ ìµœì í™”'],
                    'learning_path': ['HTML ê¸°ì´ˆ', 'CSS ìŠ¤íƒ€ì¼ë§', 'JavaScript', 'í”„ë ˆì„ì›Œí¬', 'ë°±ì—”ë“œ']
                },
                'AI': {
                    'description': 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥ê³¼ ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥, ìì—°ì–¸ì–´ì˜ ì´í•´ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì‹¤í˜„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.',
                    'features': ['ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ìì—°ì–´ì²˜ë¦¬', 'ì»´í“¨í„° ë¹„ì „'],
                    'learning_path': ['ìˆ˜í•™ ê¸°ì´ˆ', 'Python', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì‹¤ì „ í”„ë¡œì íŠ¸']
                }
            }
            
            # í‚¤ì›Œë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            info = keyword_info.get(keyword.lower(), {
                'description': f'{keyword}ì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.',
                'features': ['ê¸°ë³¸ ê°œë…', 'í•µì‹¬ ê¸°ëŠ¥', 'í™œìš© ë¶„ì•¼', 'í•™ìŠµ ë°©ë²•'],
                'learning_path': ['ê¸°ì´ˆ í•™ìŠµ', 'ì‹¤ìŠµ', 'ì‹¬í™” ê³¼ì •', 'ì‹¤ë¬´ ì ìš©']
            })
            
            # ì œëª© ìƒì„±
            title = f"{keyword} ì™„ì „ ê°€ì´ë“œ: ì‹¤ë¬´ì—ì„œ í™œìš©í•˜ëŠ” ë°©ë²•"
            
            # ë‚´ìš© ìƒì„±
            content = f"""
<h3>ğŸ¯ {keyword}ë€ ë¬´ì—‡ì¸ê°€?</h3>
<p>{info['description']}</p>

<h3>ğŸ“š ì£¼ìš” íŠ¹ì§•</h3>
<ul>
{''.join([f'<li>{feature}</li>' for feature in info['features']])}
</ul>

<h3>ğŸš€ í•™ìŠµ ë¡œë“œë§µ</h3>
<ol>
{''.join([f'<li>{step}</li>' for step in info['learning_path']])}
</ol>

<h3>ğŸ’¡ ì‹¤ë¬´ í™œìš© íŒ</h3>
<p>{keyword}ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ë ¤ë©´ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜¨ë¼ì¸ íŠœí† ë¦¬ì–¼ê³¼ ì‹¤ìŠµì„ ë³‘í–‰í•˜ì—¬ ì‹¤ë¬´ ëŠ¥ë ¥ì„ í‚¤ì›Œë³´ì„¸ìš”.</p>

<h3>ğŸ” ê´€ë ¨ ì •ë³´</h3>
"""
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
            if search_results:
                content += "<ul>"
                for result in search_results[:3]:
                    content += f'<li><strong>{result["title"]}</strong><br><small>{result["snippet"]}</small></li>'
                content += "</ul>"
            else:
                content += f"<p>{keyword}ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´ êµ¬ê¸€ ê²€ìƒ‰ì„ í™œìš©í•´ë³´ì„¸ìš”.</p>"
            
            content += f"""
<h3>ğŸŒŸ ë§ˆë¬´ë¦¬</h3>
<p>{keyword}ëŠ” ì§€ì†ì ìœ¼ë¡œ ë°œì „í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ìµœì‹  íŠ¸ë Œë“œì™€ ì—…ë°ì´íŠ¸ë¥¼ ê¾¸ì¤€íˆ í™•ì¸í•˜ë©° í•™ìŠµí•´ë³´ì„¸ìš”.</p>
            """
            
        except Exception as crawl_error:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {crawl_error}")
            # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ ì œê³µ
            title = f"{keyword} í•™ìŠµ ê°€ì´ë“œ"
            content = f"""
<h3>ğŸ¯ {keyword} í•™ìŠµí•˜ê¸°</h3>
<p>{keyword}ì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.</p>

<h3>ğŸ“š ê¸°ë³¸ í•™ìŠµ ë°©ë²•</h3>
<ul>
<li>ì˜¨ë¼ì¸ íŠœí† ë¦¬ì–¼ ì°¸ê³ </li>
<li>ì‹¤ìŠµ í”„ë¡œì íŠ¸ ì§„í–‰</li>
<li>ì»¤ë®¤ë‹ˆí‹° í™œë™</li>
<li>ìµœì‹  íŠ¸ë Œë“œ íŒŒì•…</li>
</ul>

<h3>ğŸ’¡ í•™ìŠµ íŒ</h3>
<p>ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ë©´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ì ì…ë‹ˆë‹¤.</p>
            """
        
        return jsonify({
            'success': True,
            'title': title,
            'content': content,
            'source': 'Web Crawling & Research'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/ranking')
def get_blog_ranking():
    """ë¸”ë¡œê·¸ í”Œë«í¼ ìˆœìœ„ ë°ì´í„°"""
    try:
        # ì¸ê¸° ë¸”ë¡œê·¸ í”Œë«í¼ë“¤ì˜ ê°€ìƒ ìˆœìœ„ ë°ì´í„°
        ranking_data = [
            {
                'platform': 'Tistory',
                'domain': 'tistory.com',
                'traffic_score': 95,
                'trend_score': 88,
                'user_count': 15000000,
                'growth_rate': 12.5
            },
            {
                'platform': 'Naver Blog',
                'domain': 'blog.naver.com',
                'traffic_score': 92,
                'trend_score': 85,
                'user_count': 12000000,
                'growth_rate': 8.3
            },
            {
                'platform': 'Medium',
                'domain': 'medium.com',
                'traffic_score': 88,
                'trend_score': 90,
                'user_count': 8000000,
                'growth_rate': 15.2
            },
            {
                'platform': 'Velog',
                'domain': 'velog.io',
                'traffic_score': 82,
                'trend_score': 92,
                'user_count': 3000000,
                'growth_rate': 25.7
            },
            {
                'platform': 'GitHub Pages',
                'domain': 'github.io',
                'traffic_score': 85,
                'trend_score': 87,
                'user_count': 5000000,
                'growth_rate': 18.9
            },
            {
                'platform': 'Daum Blog',
                'domain': 'blog.daum.net',
                'traffic_score': 75,
                'trend_score': 70,
                'user_count': 6000000,
                'growth_rate': 2.1
            }
        ]
        
        # ìˆœìœ„ ê³„ì‚° (ì¢…í•© ì ìˆ˜)
        for item in ranking_data:
            item['total_score'] = (item['traffic_score'] * 0.4 + 
                                 item['trend_score'] * 0.4 + 
                                 item['growth_rate'] * 0.2)
        
        # ì´ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        ranking_data.sort(key=lambda x: x['total_score'], reverse=True)
        
        # ìˆœìœ„ ì¶”ê°€
        for i, item in enumerate(ranking_data, 1):
            item['rank'] = i
        
        return jsonify({
            'success': True,
            'ranking': ranking_data
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(debug=True, host='0.0.0.0', port=port) 